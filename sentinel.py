# filename: sentinel.py

import asyncio
import logging
from datetime import datetime, timedelta, timezone
from typing import Optional, Dict, Any, List

import httpx
from sqlalchemy.orm import Session

# Ù…Ù‚Ø§Ù…ÛŒ Ø§Ù…Ù¾ÙˆØ±Ù¹Ø³
from database_crud import update_news_cache_in_db, get_cached_news
from models import SessionLocal
# Ù…Ø±Ú©Ø²ÛŒ Ú©Ù†ÙÛŒÚ¯Ø±ÛŒØ´Ù† Ù…Ø§ÚˆÛŒÙˆÙ„Ø² Ø³Û’ Ø³ÛŒÙ¹Ù†Ú¯Ø² Ø¯Ø±Ø¢Ù…Ø¯ Ú©Ø±ÛŒÚº
from config import api_settings, news_settings, trading_settings

logger = logging.getLogger(__name__)

# --- Ú©Ù†ÙÛŒÚ¯Ø±ÛŒØ´Ù† Ø³Û’ Ù…Ø³ØªÙ‚Ù„ Ø§Ù‚Ø¯Ø§Ø± ---
MARKETAUX_API_KEY = api_settings.MARKETAUX_API_KEY
HIGH_IMPACT_KEYWORDS = news_settings.HIGH_IMPACT_KEYWORDS

def _get_news_symbols() -> str:
    """
    Ú©Ù†ÙÛŒÚ¯Ø±ÛŒØ´Ù† Ø³Û’ ØªÙ…Ø§Ù… Ù¹Ø±ÛŒÚˆÙ†Ú¯ Ø¬ÙˆÚ‘ÙˆÚº Ú©Ùˆ Ø§Ú©Ù¹Ú¾Ø§ Ú©Ø±Ú©Û’ Ø§ÛŒÚ© Ù…Ù†ÙØ±Ø¯ Ø¹Ù„Ø§Ù…ØªÙˆÚº Ú©ÛŒ ÙÛØ±Ø³Øª Ø¨Ù†Ø§ØªØ§ ÛÛ’Û”
    """
    all_pairs = (
        trading_settings.WEEKDAY_PRIMARY +
        trading_settings.WEEKDAY_BACKUP +
        trading_settings.WEEKEND_PRIMARY +
        trading_settings.WEEKEND_BACKUP
    )
    
    unique_symbols = set()
    for pair in all_pairs:
        parts = pair.split('/')
        unique_symbols.update(p.strip() for p in parts)
        
    # Ø§Ø¶Ø§ÙÛŒ Ø§ÛÙ… Ø¹Ù„Ø§Ù…ØªÛŒÚº Ø´Ø§Ù…Ù„ Ú©Ø±ÛŒÚº
    unique_symbols.update(['TSLA', 'AMZN', 'MSFT', 'GOOGL'])
    
    return ",".join(sorted(list(unique_symbols)))

async def _fetch_news_from_marketaux(client: httpx.AsyncClient) -> Optional[Dict[str, Any]]:
    """MarketAux API Ø³Û’ ØªØ§Ø²Û ØªØ±ÛŒÙ† Ø®Ø¨Ø±ÛŒÚº Ø­Ø§ØµÙ„ Ú©Ø±ØªØ§ ÛÛ’Û”"""
    if not MARKETAUX_API_KEY:
        logger.error("MarketAux API Ú©ÛŒ Ú©Ù„ÛŒØ¯ (MARKETAUX_API_KEY) Ø³ÛŒÙ¹ Ù†ÛÛŒÚº ÛÛ’Û” Ø®Ø¨Ø±ÛŒÚº Ø­Ø§ØµÙ„ Ù†ÛÛŒÚº Ú©ÛŒ Ø¬Ø§ Ø³Ú©ØªÛŒÚºÛ”")
        return None
        
    symbols_str = _get_news_symbols()
    url = (f"https://api.marketaux.com/v1/news/all?symbols={symbols_str}"
           f"&filter_entities=true&language=en&limit=100&api_token={MARKETAUX_API_KEY}")
    
    try:
        logger.info("MarketAux API Ø³Û’ Ø®Ø¨Ø±ÛŒÚº Ø­Ø§ØµÙ„ Ú©ÛŒ Ø¬Ø§ Ø±ÛÛŒ ÛÛŒÚº...")
        response = await client.get(url, timeout=20)
        response.raise_for_status()
        data = response.json()
        logger.info(f"MarketAux API Ø³Û’ Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒ Ø³Û’ {len(data.get('data', []))} Ø®Ø¨Ø±ÛŒÚº Ø­Ø§ØµÙ„ Ú©ÛŒ Ú¯Ø¦ÛŒÚºÛ”")
        return data
    except httpx.HTTPStatusError as e:
        logger.error(f"MarketAux API Ø³Û’ HTTP Ø®Ø±Ø§Ø¨ÛŒ: {e.response.status_code} - {e.response.text}")
    except Exception as e:
        logger.error(f"MarketAux Ø³Û’ Ø®Ø¨Ø±ÛŒÚº Ø­Ø§ØµÙ„ Ú©Ø±Ù†Û’ Ù…ÛŒÚº ØºÛŒØ± Ù…ØªÙˆÙ‚Ø¹ Ø®Ø±Ø§Ø¨ÛŒ: {e}", exc_info=True)
    
    return None

async def update_economic_calendar_cache():
    """
    Ù…Ø§Ø±Ú©ÛŒÙ¹ Ú©ÛŒ Ø®Ø¨Ø±ÙˆÚº Ú©Ùˆ Ø§Ù¾ ÚˆÛŒÙ¹ Ú©Ø±ØªØ§ ÛÛ’ Ø§ÙˆØ± Ø§Ù†ÛÛŒÚº Ø¹Ù„Ø§Ù…Øª Ú©Û’ Ù„Ø­Ø§Ø¸ Ø³Û’ Ú¯Ø±ÙˆÙ¾ Ú©Ø±Ú©Û’ ÚˆÛŒÙ¹Ø§ Ø¨ÛŒØ³ Ù…ÛŒÚº Ú©ÛŒØ´ Ú©Ø±ØªØ§ ÛÛ’Û”
    """
    logger.info("ğŸ“° Ø®Ø¨Ø±ÙˆÚº Ú©Ø§ Ú©ÛŒØ´ Ø§Ù¾ ÚˆÛŒÙ¹ Ú©Ø±Ù†Û’ Ú©Ø§ Ú©Ø§Ù… Ø´Ø±ÙˆØ¹ ÛÙˆ Ø±ÛØ§ ÛÛ’...")
    
    async with httpx.AsyncClient() as client:
        news_data = await _fetch_news_from_marketaux(client)

    if not (news_data and 'data' in news_data and news_data['data']):
        logger.warning("MarketAux Ø³Û’ Ú©ÙˆØ¦ÛŒ Ø®Ø¨Ø± Ù†ÛÛŒÚº Ù…Ù„ÛŒ ÛŒØ§ Ø¬ÙˆØ§Ø¨ Ø®Ø§Ù„ÛŒ ØªÚ¾Ø§Û” Ú©ÛŒØ´ Ø§Ù¾ ÚˆÛŒÙ¹ Ù†ÛÛŒÚº ÛÙˆØ§Û”")
        return

    categorized_articles = {}
    for item in news_data['data']:
        title = item.get('title', '').lower()
        snippet = item.get('snippet', '').lower()
        content = f"{title} {snippet}"
        
        impact = "Low"
        # Ø§Ø¹Ù„ÛŒÙ° Ø§Ø«Ø± ÙˆØ§Ù„Û’ Ù…Ø·Ù„ÙˆØ¨Û Ø§Ù„ÙØ§Ø¸ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ Ù¾Ø± Ø§Ø«Ø± Ú©Ø§ ØªØ¹ÛŒÙ† Ú©Ø±ÛŒÚº
        for currency, keywords in HIGH_IMPACT_KEYWORDS.items():
            if any(keyword in content for keyword in keywords):
                impact = "High"
                break

        article_data = {
            'title': item.get('title'),
            'url': item.get('url'),
            'source': item.get('source'),
            'snippet': item.get('snippet'),
            'published_at': item.get('published_at'),
            'impact': impact,
            'entities': [entity.get('symbol') for entity in item.get('entities', [])]
        }
        
        for entity_symbol in article_data['entities']:
            if entity_symbol not in categorized_articles:
                categorized_articles[entity_symbol] = []
            categorized_articles[entity_symbol].append(article_data)
    
    db = SessionLocal()
    try:
        update_news_cache_in_db(db, {"articles_by_symbol": categorized_articles})
        logger.info(f"ğŸ“° Ø®Ø¨Ø±ÙˆÚº Ú©Ø§ Ú©ÛŒØ´ Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒ Ø³Û’ {len(categorized_articles)} Ø¹Ù„Ø§Ù…ØªÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ø§Ù¾ ÚˆÛŒÙ¹ ÛÙˆ Ú¯ÛŒØ§Û”")
    finally:
        db.close()

def _parse_datetime_string(datetime_str: str) -> Optional[datetime]:
    """
    Ù…Ø®ØªÙ„Ù ÙØ§Ø±Ù…ÛŒÙ¹Ø³ Ù…ÛŒÚº Ø¢Ù†Û’ ÙˆØ§Ù„ÛŒ ØªØ§Ø±ÛŒØ® Ú©ÛŒ Ø³Ù¹Ø±Ù†Ú¯ Ú©Ùˆ Ù…Ø­ÙÙˆØ¸ Ø·Ø±ÛŒÙ‚Û’ Ø³Û’ Ù¾Ø§Ø±Ø³ Ú©Ø±ØªØ§ ÛÛ’
    Ø§ÙˆØ± Ø§Ø³Û’ Ù¹Ø§Ø¦Ù… Ø²ÙˆÙ† Ø³Û’ Ø¢Ú¯Ø§Û (timezone-aware) UTC datetime Ø¢Ø¨Ø¬ÛŒÚ©Ù¹ Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±ØªØ§ ÛÛ’Û”
    """
    if not datetime_str:
        return None
    try:
        # ISO 8601 ÙØ§Ø±Ù…ÛŒÙ¹ Ú©Ùˆ ÛÛŒÙ†ÚˆÙ„ Ú©Ø±Ù†Û’ Ú©ÛŒ Ú©ÙˆØ´Ø´ Ú©Ø±ÛŒÚº
        dt = datetime.fromisoformat(datetime_str.replace('Z', '+00:00'))
        # Ø§Ú¯Ø± Ù¹Ø§Ø¦Ù… Ø²ÙˆÙ† Ú©ÛŒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù†Û ÛÙˆÚº ØªÙˆ Ø§Ø³Û’ UTC Ø³Ù…Ø¬Ú¾ÛŒÚº
        return dt.astimezone(timezone.utc) if dt.tzinfo else dt.replace(tzinfo=timezone.utc)
    except (ValueError, TypeError):
        logger.warning(f"Ø®Ø¨Ø± Ú©ÛŒ ØªØ§Ø±ÛŒØ® Ú©Ùˆ Ù¾Ø§Ø±Ø³ Ù†ÛÛŒÚº Ú©ÛŒØ§ Ø¬Ø§ Ø³Ú©Ø§: '{datetime_str}'")
        return None

async def get_news_analysis_for_symbol(symbol: str) -> Dict[str, Any]:
    """
    Ú©ÛŒØ´ Ø´Ø¯Û Ø®Ø¨Ø±ÙˆÚº Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ Ù¾Ø± Ú©Ø³ÛŒ Ù…Ø®ØµÙˆØµ Ø¹Ù„Ø§Ù…Øª Ú©Û’ Ù„ÛŒÛ’ Ø®Ø¨Ø±ÙˆÚº Ú©Û’ Ø§Ø«Ø±Ø§Øª Ú©Ø§ ØªÛŒØ²ÛŒ Ø³Û’ ØªØ¬Ø²ÛŒÛ Ú©Ø±ØªØ§ ÛÛ’Û”
    """
    db = SessionLocal()
    try:
        all_news_content = get_cached_news(db)
        if not (all_news_content and 'articles_by_symbol' in all_news_content):
            return {"impact": "Clear", "reason": "Ø®Ø¨Ø±ÙˆÚº Ú©Ø§ Ú©ÛŒØ´ Ø®Ø§Ù„ÛŒ ÛŒØ§ ØºÙ„Ø· ÙØ§Ø±Ù…ÛŒÙ¹ Ù…ÛŒÚº ÛÛ’Û”"}
    finally:
        db.close()

    symbol_parts = [s.strip().upper() for s in symbol.split('/')]
    
    relevant_articles = []
    for part in symbol_parts:
        relevant_articles.extend(all_news_content['articles_by_symbol'].get(part, []))
    
    if not relevant_articles:
        return {"impact": "Clear", "reason": "Ø§Ø³ Ø¹Ù„Ø§Ù…Øª Ú©Û’ Ù„ÛŒÛ’ Ú©ÙˆØ¦ÛŒ Ù…ØªØ¹Ù„Ù‚Û Ø®Ø¨Ø± Ù†ÛÛŒÚº Ù…Ù„ÛŒÛ”"}

    now_utc = datetime.now(timezone.utc)
    
    for article in relevant_articles:
        if article.get('impact') != "High":
            continue

        published_time = _parse_datetime_string(article.get('published_at'))
        if not published_time:
            continue
            
        # Ú†ÛŒÚ© Ú©Ø±ÛŒÚº Ú©Û Ø¢ÛŒØ§ Ø®Ø¨Ø± Ø­Ø§Ù„ ÛÛŒ Ù…ÛŒÚº (Ù¾Ú†Ú¾Ù„Û’ 1 Ú¯Ú¾Ù†Ù¹Û’ Ù…ÛŒÚº) Ø¬Ø§Ø±ÛŒ ÛÙˆØ¦ÛŒ ÛÛ’
        # ÛŒØ§ Ø¬Ù„Ø¯ ÛÛŒ (Ø§Ú¯Ù„Û’ 4 Ú¯Ú¾Ù†Ù¹ÙˆÚº Ù…ÛŒÚº) Ø¢Ù†Û’ ÙˆØ§Ù„ÛŒ ÛÛ’
        if now_utc - timedelta(hours=1) <= published_time <= now_utc + timedelta(hours=4):
            return {
                "impact": "High",
                "reason": f"Ø§ÛŒÚ© Ø§Ø¹Ù„ÛŒÙ° Ø§Ø«Ø± ÙˆØ§Ù„ÛŒ Ø®Ø¨Ø± Ù…Ù„ÛŒ: '{article.get('title', '')[:60]}...'"
            }
            
    return {"impact": "Clear", "reason": "Ø§Ø³ Ø¹Ù„Ø§Ù…Øª Ú©Û’ Ù„ÛŒÛ’ Ú©ÙˆØ¦ÛŒ Ø­Ø§Ù„ÛŒÛ ÛŒØ§ Ø¢Ù†Û’ ÙˆØ§Ù„ÛŒ Ø§Ø¹Ù„ÛŒÙ° Ø§Ø«Ø± ÙˆØ§Ù„ÛŒ Ø®Ø¨Ø± Ù†ÛÛŒÚº Ù…Ù„ÛŒÛ”"}

async def check_news_at_time_of_trade(symbol: str, trade_start_time: datetime, trade_end_time: datetime) -> bool:
    """
    ÛŒÛ Ú†ÛŒÚ© Ú©Ø±ØªØ§ ÛÛ’ Ú©Û Ø¢ÛŒØ§ Ú©Ø³ÛŒ Ù¹Ø±ÛŒÚˆ Ú©Û’ Ø¯ÙˆØ±Ø§Ù† Ú©ÙˆØ¦ÛŒ Ø§Ø¹Ù„ÛŒÙ° Ø§Ø«Ø± ÙˆØ§Ù„ÛŒ Ø®Ø¨Ø± Ø¬Ø§Ø±ÛŒ ÛÙˆØ¦ÛŒ ØªÚ¾ÛŒÛ”
    """
    db = SessionLocal()
    try:
        all_news_content = get_cached_news(db)
        if not (all_news_content and 'articles_by_symbol' in all_news_content):
            return False
    finally:
        db.close()

    symbol_parts = [s.strip().upper() for s in symbol.split('/')]
    
    relevant_articles = []
    for part in symbol_parts:
        relevant_articles.extend(all_news_content['articles_by_symbol'].get(part, []))
    
    if not relevant_articles:
        return False

    # Ù¹Ø±ÛŒÚˆ Ú©Û’ Ø§ÙˆÙ‚Ø§Øª Ú©Ùˆ Ù¹Ø§Ø¦Ù… Ø²ÙˆÙ† Ø³Û’ Ø¢Ú¯Ø§Û Ø¨Ù†Ø§Ø¦ÛŒÚº (UTC)
    start_time_utc = trade_start_time.replace(tzinfo=timezone.utc)
    end_time_utc = trade_end_time.replace(tzinfo=timezone.utc)

    for article in relevant_articles:
        if article.get('impact') != "High":
            continue

        published_time = _parse_datetime_string(article.get('published_at'))
        if not published_time:
            continue
            
        if start_time_utc <= published_time <= end_time_utc:
            logger.info(f"Ù¹Ø±ÛŒÚˆ {symbol} Ú©Û’ Ø¯ÙˆØ±Ø§Ù† Ø§ÛŒÚ© Ø§Ø¹Ù„ÛŒÙ° Ø§Ø«Ø± ÙˆØ§Ù„ÛŒ Ø®Ø¨Ø± Ù…Ù„ÛŒ: '{article.get('title', '')[:60]}...'")
            return True
            
    return False
    
