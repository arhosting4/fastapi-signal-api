# filename: sentinel.py

import os
import httpx
import asyncio
import logging
from sqlalchemy.orm import Session
from typing import Optional, Dict, Any, List
from datetime import datetime, timedelta, timezone

# Ù…Ù‚Ø§Ù…ÛŒ Ø§Ù…Ù¾ÙˆØ±Ù¹Ø³
from database_crud import update_news_cache_in_db, get_cached_news
from models import SessionLocal
from config import HIGH_IMPACT_KEYWORDS

logger = logging.getLogger(__name__)
MARKETAUX_API_KEY = os.getenv("MARKETAUX_API_KEY")

# =====================================================================================
async def fetch_news_from_marketaux(client: httpx.AsyncClient) -> Optional[Dict[str, Any]]:
    """
    MarketAux API Ø³Û’ Ø®Ø¨Ø±ÛŒÚº Ø­Ø§ØµÙ„ Ú©Ø±ØªØ§ ÛÛ’
    """
    if not MARKETAUX_API_KEY:
        logger.error("âŒ MarketAux API Ú©Ù„ÛŒØ¯ Ù†ÛÛŒÚº Ù…Ù„ÛŒ (MARKETAUX_API_KEY)")
        return None

    url = (
        f"https://api.marketaux.com/v1/news/all"
        f"?symbols=TSLA,AMZN,MSFT,GOOGL,XAU,EUR,GBP,JPY,CHF,CAD,AUD,NZD,BTC,ETH"
        f"&filter_entities=true&language=en&limit=100"
        f"&api_token={MARKETAUX_API_KEY}"
    )

    try:
        logger.info("ğŸŒ MarketAux API Ø³Û’ Ø®Ø¨Ø±ÛŒÚº Ø­Ø§ØµÙ„ Ú©ÛŒ Ø¬Ø§ Ø±ÛÛŒ ÛÛŒÚº...")
        response = await client.get(url, timeout=20)
        response.raise_for_status()
        data = response.json()
        logger.info(f"âœ… {len(data.get('data', []))} Ø®Ø¨Ø±ÛŒÚº Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒ Ø³Û’ Ø­Ø§ØµÙ„ ÛÙˆ Ú¯Ø¦ÛŒÚº")
        return data
    except httpx.HTTPStatusError as e:
        logger.error(f"âŒ HTTP Ø®Ø±Ø§Ø¨ÛŒ: {e.response.status_code} - {e.response.text}")
    except Exception as e:
        logger.error(f"âŒ Ø®Ø¨Ø±ÛŒÚº Ø­Ø§ØµÙ„ Ú©Ø±Ù†Û’ Ù…ÛŒÚº Ø®Ø±Ø§Ø¨ÛŒ: {e}", exc_info=True)
    return None

# =====================================================================================
def check_news_at_time_of_trade(db: Session, symbol: str, trade_time: datetime) -> str:
    """
    Ø¯ÛŒÛ’ Ú¯Ø¦Û’ symbol Ø§ÙˆØ± ÙˆÙ‚Øª Ù¾Ø± Ù†ÛŒÙˆØ² Ø§Ù…Ù¾ÛŒÚ©Ù¹ Ú†ÛŒÚ© Ú©Ø±ØªØ§ ÛÛ’Û”
    """
    try:
        cached = get_cached_news(db)
        news_list = cached.get("data", []) if cached else []

        impact_found = "none"
        window = timedelta(minutes=30)
        for item in news_list:
            try:
                pub_time = datetime.fromisoformat(item["published_at"].replace("Z", "+00:00"))
                if abs(pub_time - trade_time) <= window:
                    title = item.get("title", "").lower()
                    if any(keyword.lower() in title for keyword in HIGH_IMPACT_KEYWORDS):
                        impact_found = "positive" if "gain" in title or "up" in title else "negative"
                        break
            except Exception as e:
                logger.warning(f"âš ï¸ Ø§ÛŒÚ© Ù†ÛŒÙˆØ² item Ú©Ùˆ parse Ú©Ø±Ù†Û’ Ù…ÛŒÚº Ø®Ø±Ø§Ø¨ÛŒ: {e}")
        return impact_found
    except Exception as e:
        logger.error(f"âŒ Ù†ÛŒÙˆØ² Ú†ÛŒÚ© Ú©Ø±ØªÛ’ ÙˆÙ‚Øª Ø®Ø±Ø§Ø¨ÛŒ: {e}", exc_info=True)
        return "none"

# =====================================================================================
async def refresh_news_and_cache():
    """
    MarketAux API Ø³Û’ Ø®Ø¨Ø±ÛŒÚº Ø­Ø§ØµÙ„ Ú©Ø±ØªØ§ ÛÛ’ Ø§ÙˆØ± DB Ù…ÛŒÚº cache Ú©Ø±ØªØ§ ÛÛ’
    """
    try:
        async with httpx.AsyncClient() as client:
            news_data = await fetch_news_from_marketaux(client)
            if news_data:
                db: Session = SessionLocal()
                update_news_cache_in_db(db, news_data)
                db.close()
    except Exception as e:
        logger.error(f"âŒ Cache refresh Ú©Ø±ØªÛ’ ÙˆÙ‚Øª Ø®Ø±Ø§Ø¨ÛŒ: {e}", exc_info=True)
